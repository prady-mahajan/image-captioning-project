{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f70b6e6-860e-4421-b2d5-3d53f471265d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5415e5dd-9d80-4369-a7aa-51a34d8fee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "with open(\"../data/flickr_test_images.json\", \"r\") as f:\n",
    "    test_image_filenames = json.load(f)\n",
    "\n",
    "image_dir = \"../data/Flickr8k_Dataset/Flicker8k_Dataset\"\n",
    "test_image_paths = [os.path.join(image_dir, img_name) for img_name in test_image_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80b077ee-bae1-488f-b1a9-fc91d7deda27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1000 [00:00<?, ?it/s]`cache.key_cache[idx]` is deprecated and will be removed in v4.56.0. Use `cache.layers[idx].keys` instead.\n",
      "`cache.value_cache[idx]` is deprecated and will be removed in v4.56.0. Use `cache.layers[idx].values` instead.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [27:44<00:00,  1.66s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "blip_predictions = {}\n",
    "\n",
    "for img_path in tqdm(test_image_paths):\n",
    "    try:\n",
    "        raw_image = Image.open(img_path).convert('RGB')\n",
    "        inputs = processor(raw_image, return_tensors=\"pt\").to(device)\n",
    "        out = model.generate(**inputs)\n",
    "        caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "        img_name = os.path.basename(img_path)\n",
    "        blip_predictions[img_name] = caption\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {img_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "419b255f-a41b-4ac9-80c9-5e0fb48bb479",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/flickr_encoded_captions.json\", \"r\") as f:\n",
    "    gt_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10d5f36f-edf8-417e-b28a-e85001a0ccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/flickr_vocab.json\", \"r\") as f:\n",
    "    vocab_data = json.load(f)\n",
    "    idx2word = {int(k): v for k, v in vocab_data['idx2word'].items()}  \n",
    "\n",
    "with open(\"../data/flickr_encoded_captions.json\", \"r\") as f:\n",
    "    gt_encoded = json.load(f)\n",
    "\n",
    "def decode_caption(encoded_caption, idx2word):\n",
    "    decoded = []\n",
    "    for idx in encoded_caption:\n",
    "        word = idx2word.get(idx, '<unk>')\n",
    "        if word not in ['<pad>', '<start>', '<end>']:  \n",
    "            decoded.append(word)\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72a427e6-ab64-4cef-af05-8a6e99ead026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:05<00:00, 179.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU-1: 0.6235\n",
      "BLEU-2: 0.4824\n",
      "BLEU-3: 0.3553\n",
      "BLEU-4: 0.2515\n",
      "Avg BLEU (1-4): 0.4282\n",
      "METEOR: 0.4137\n",
      "\n",
      "Number of evaluated pairs: 1000\n",
      "\n",
      "Sample comparison:\n",
      "\n",
      "Image: 3385593926_d3e9c21170.jpg\n",
      "Ground Truth: ['the', 'dogs', 'are', 'in', 'the', 'snow', 'in', 'front', 'of', 'a']...\n",
      "Prediction: ['two', 'dogs', 'playing', 'in', 'the', 'snow']...\n",
      "\n",
      "Image: 2677656448_6b7e7702af.jpg\n",
      "Ground Truth: ['a', 'brown', 'and', 'white', 'dog', 'swimming', 'towards', 'some', 'in', 'the']...\n",
      "Prediction: ['a', 'man', 'in', 'a', 'pool', 'with', 'a', 'dog']...\n",
      "\n",
      "Image: 311146855_0b65fdb169.jpg\n",
      "Ground Truth: ['a', 'man', 'and', 'a', 'woman', 'in', 'festive', 'costumes', 'dancing']...\n",
      "Prediction: ['man', 'wearing', 'a', 'yellow', 'and', 'green', 'costume']...\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "references = []\n",
    "hypotheses = []\n",
    "meteor_scores = []\n",
    "\n",
    "for img_name, pred_caption in tqdm(blip_predictions.items()):\n",
    "    encoded_gt_captions = gt_encoded.get(img_name, [])\n",
    "    if not encoded_gt_captions:\n",
    "        continue\n",
    "    \n",
    "    decoded_gt_captions = [decode_caption(encoded_cap, idx2word) for encoded_cap in encoded_gt_captions]\n",
    "    \n",
    "    tokenized_refs = [[word.lower() for word in ref_caption] for ref_caption in decoded_gt_captions]\n",
    "    \n",
    "    tokenized_pred = pred_caption.strip().lower().split()\n",
    "    \n",
    "    references.append(tokenized_refs)\n",
    "    hypotheses.append(tokenized_pred)\n",
    "    \n",
    "    meteor_per_ref = []\n",
    "    for ref in tokenized_refs:\n",
    "        try:\n",
    "            meteor_per_ref.append(meteor_score([ref], tokenized_pred))\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    if meteor_per_ref:\n",
    "        meteor_scores.append(max(meteor_per_ref))\n",
    "\n",
    "smoothie = SmoothingFunction().method4\n",
    "bleu1 = corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0), smoothing_function=smoothie)\n",
    "bleu2 = corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothie)\n",
    "bleu3 = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothie)\n",
    "bleu4 = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)\n",
    "avg_bleu = (bleu1 + bleu2 + bleu3 + bleu4) / 4\n",
    "avg_meteor = sum(meteor_scores) / len(meteor_scores) if meteor_scores else 0.0\n",
    "\n",
    "print(f\"\\nBLEU-1: {bleu1:.4f}\")\n",
    "print(f\"BLEU-2: {bleu2:.4f}\")\n",
    "print(f\"BLEU-3: {bleu3:.4f}\")\n",
    "print(f\"BLEU-4: {bleu4:.4f}\")\n",
    "print(f\"Avg BLEU (1-4): {avg_bleu:.4f}\")\n",
    "print(f\"METEOR: {avg_meteor:.4f}\")\n",
    "\n",
    "print(f\"\\nNumber of evaluated pairs: {len(references)}\")\n",
    "print(f\"\\nSample comparison:\")\n",
    "for i in range(min(3, len(references))):\n",
    "    img_name = list(blip_predictions.keys())[i]\n",
    "    print(f\"\\nImage: {img_name}\")\n",
    "    print(f\"Ground Truth: {references[i][0][:10]}...\")  \n",
    "    print(f\"Prediction: {hypotheses[i][:10]}...\")      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
